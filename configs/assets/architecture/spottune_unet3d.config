from functools import partial

import numpy as np
import torch

from dl2021.torch.module.agent_net_3d import resnet
from dl2021.torch.module.spottune_unet_3d_layerwise import UNet3D
from dpipe.torch.functional import weighted_cross_entropy_with_logits
from dpipe.predict import add_extract_dims, divisible_shape
from dl2021.batch_iter import slicewise, SPATIAL_DIMS
from dl2021.torch.model import train_step_spottune, inference_step_spottune
from dl2021.metric import compute_metrics_probably_with_ids_spottune
from dpipe.predict.shape import patches_grid
from dpipe.train import Checkpoints
from dpipe.train.policy import Schedule


# loss
criterion = weighted_cross_entropy_with_logits

# model
n_filters = 16
architecture_main = UNet3D(n_chans_in=n_chans_in, n_chans_out=n_chans_out, n_filters_init=n_filters)
architecture_policy = resnet(num_class=64)

x_patch_size = y_patch_size = np.array([256, 256, 256])
batch_size = 16

# optimizer
batches_per_epoch = 100
n_epochs = 60

lr_init_main = 1e-3
lr_main = Schedule(initial=lr_init_main, epoch2value_multiplier={45: 0.1, })

lr_init_policy = 0.01
lr_policy = Schedule(initial=lr_init_policy, epoch2value_multiplier={45: 0.1, })

optimizer_main = torch.optim.SGD(
    architecture_main.parameters(),
    lr=lr_init_main,
    momentum=0.9,
    nesterov=True,
    weight_decay=0
)

optimizer_policy = torch.optim.SGD(
    architecture_policy.parameters(),
    lr=lr_init_policy,
    momentum=0.9,
    nesterov=True,
    weight_decay=0.001
)

# spottune kwargs
use_gumbel_inference = False
temperature = 0.1
k_reg = 0
reg_mode = 'l1'
with_source = False
k_reg_source = None
alpha_l2sp = None

train_kwargs = dict(lr_main=lr_main, lr_policy=lr_policy, k_reg=k_reg, k_reg_source=k_reg_source, reg_mode=reg_mode,
                    architecture_main=architecture_main, architecture_policy=architecture_policy,
                    temperature=temperature, with_source=with_source, optimizer_main=optimizer_main,
                    optimizer_policy=optimizer_policy, criterion=criterion, alpha_l2sp=alpha_l2sp)

# Checkpoints
checkpoints = Checkpoints(checkpoints_path, {
    **{k: v for k, v in train_kwargs.items() if isinstance(v, Policy)},
    'model_main.pth': architecture_main, 'model_policy.pth': architecture_policy,
    'optimizer_main.pth': optimizer_main, 'optimizer_policy.pth': optimizer_policy
})

# training
train_model = train(
    train_step=train_step_spottune,
    batch_iter=batch_iter,
    n_epochs=n_epochs,
    logger=logger,
    checkpoints=checkpoints,
    validate=validate_step,
    **train_kwargs
)

# validation
validate_step = partial(compute_metrics_probably_with_ids_spottune, predict=val_predict,
                        load_x=load_x, load_y=load_y, ids=val_ids, metrics=val_metrics,
                        architecture_main=architecture_main)

pred_patch_size = np.array([32,]*3)
pred_patch_stride = np.array([16,]*3)

# predict
@add_extract_dims(2)
@patches_grid(pred_patch_size, pred_patch_stride, axis=SPATIAL_DIMS)
@divisible_shape(divisor=[8]*3 , padding_values=np.min, axis=SPATIAL_DIMS)
def predict(image):
    return inference_step_spottune(image, architecture_main=architecture_main, architecture_policy=architecture_policy,
                                   activation=torch.sigmoid, temperature=temperature, use_gumbel=use_gumbel_inference)
